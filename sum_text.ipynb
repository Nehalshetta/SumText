{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nehalshetta/SumText/blob/model/sum_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ph7P8nj7GXS"
      },
      "source": [
        "# <span style=\"color:black\">I.Dataset Preprocessing</span>\n",
        "## <span style=\"color:black\">This code block <span style=\"color:green\"> imports </span>every Python library used in the code</span>\n",
        "\n",
        "This code block imports various libraries and modules for text processing and analysis tasks. It includes **pandas** for data manipulation, **re** and **string** for regular expressions and string operations, **unicodedata** for working with Unicode characters, **nltk** for natural language processing, **stopwords** for removing common words, **word_tokenize** for tokenizing text into words, **WordNetLemmatizer** for lemmatization, **PorterStemmer** for stemming, and **BeautifulSoup** for web scraping. These imports provide the necessary functionality to clean and preprocess text data, tokenize words, and perform other text-related tasks."
      ],
      "id": "4Ph7P8nj7GXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixNrSkw3pEzL",
        "outputId": "8a5225e2-1806-4f8f-e1ee-008290df152c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "id": "ixNrSkw3pEzL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "debd4912",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Conv1D, MaxPooling1D, Flatten, Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import to_categorical"
      ],
      "id": "debd4912"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0J5aRZws5gi"
      },
      "source": [
        "The code **nltk.download('punkt')** downloads resources for NLTK's **word_tokenize** function, used for tokenizing text.\n",
        "**nltk.download('stopwords')** downloads a collection of common stopwords, while **nltk.download('wordnet')** downloads the WordNet lexical database. These downloads ensure that the necessary resources are available for text preprocessing and analysis tasks using NLTK."
      ],
      "id": "P0J5aRZws5gi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNtYPtFEvML3"
      },
      "source": [
        "- Creates a directory named **.kaggle** in the user's home directory.\n",
        "- Copies the **kaggle.json** file to the **.kaggle** directory, which is used for authentication with the Kaggle API.\n",
        "- Sets the permissions of the **kaggle.json** file to read and write only for the owner.\n",
        "- Downloads a specific dataset from Kaggle using the Kaggle CLI.\n",
        "- Extracts the contents of the downloaded zip file into a directory named **dataset-folder**."
      ],
      "id": "RNtYPtFEvML3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Iz0oiFpp5QV"
      },
      "outputs": [],
      "source": [
        "# Define dataset directory\n",
        "dataset_directory = '/content/dataset-folder/cnn_dailymail/'"
      ],
      "id": "7Iz0oiFpp5QV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5692ee84",
        "outputId": "968dfc97-c1bf-43f3-a604-993eec220e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading newspaper-text-summarization-cnn-dailymail.zip to /content\n",
            " 97% 487M/503M [00:03<00:00, 168MB/s]\n",
            "100% 503M/503M [00:03<00:00, 157MB/s]\n",
            "Archive:  /content/newspaper-text-summarization-cnn-dailymail.zip\n",
            "  inflating: dataset-folder/cnn_dailymail/test.csv  \n",
            "  inflating: dataset-folder/cnn_dailymail/train.csv  \n",
            "  inflating: dataset-folder/cnn_dailymail/validation.csv  \n"
          ]
        }
      ],
      "source": [
        "# download the dataset\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d gowrishankarp/newspaper-text-summarization-cnn-dailymail\n",
        "\n",
        "!unzip /content/newspaper-text-summarization-cnn-dailymail.zip -d dataset-folder"
      ],
      "id": "5692ee84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHc6Yvata2wC"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train = pd.read_csv(dataset_directory + 'train.csv')\n",
        "validation = pd.read_csv(dataset_directory + 'validation.csv')\n",
        "test = pd.read_csv(dataset_directory + 'test.csv')"
      ],
      "id": "vHc6Yvata2wC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce049ffd"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns\n",
        "columns_to_keep = ['article', 'highlights']\n",
        "train = train[columns_to_keep]\n",
        "validation = validation[columns_to_keep]\n",
        "test = test[columns_to_keep]"
      ],
      "id": "ce049ffd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBgkhvCNvt02"
      },
      "source": [
        "This function is designed to preprocess and clean text data by removing HTML tags, converting to lowercase, removing accents, tokenizing, removing stopwords and non-alphabetic tokens, and lemmatizing the words. It can be used as a preprocessing step for text analysis tasks."
      ],
      "id": "dBgkhvCNvt02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c2bb59d"
      },
      "outputs": [],
      "source": [
        "# Define function to clean text data\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags and other markup\n",
        "    clean_text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Convert to lowercase and remove accents\n",
        "    clean_text = clean_text.lower()\n",
        "    clean_text = unicodedata.normalize('NFKD', clean_text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(clean_text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if not t in stop_words and t.isalpha()]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    clean_text = ' '.join(tokens)\n",
        "\n",
        "    return clean_text\n"
      ],
      "id": "2c2bb59d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dab7b32e"
      },
      "outputs": [],
      "source": [
        "# Clean the text in the train dataset\n",
        "train['article'] = train['article'].apply(clean_text)\n",
        "train['highlights'] = train['highlights'].apply(clean_text)"
      ],
      "id": "dab7b32e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad54a330"
      },
      "outputs": [],
      "source": [
        "# Clean the text in the validation dataset\n",
        "validation['article'] = validation['article'].apply(clean_text)\n",
        "validation['highlights'] = validation['highlights'].apply(clean_text)"
      ],
      "id": "ad54a330"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ddb8a0"
      },
      "outputs": [],
      "source": [
        "# Clean the text in the test dataset\n",
        "test['article'] = test['article'].apply(clean_text)\n",
        "test['highlights'] = test['highlights'].apply(clean_text)"
      ],
      "id": "05ddb8a0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a923f44e"
      },
      "outputs": [],
      "source": [
        "# Save cleaned datasets into new CSV files\n",
        "train.to_csv(dataset_directory + 'cleaned_train.csv', index=False)\n",
        "validation.to_csv(dataset_directory + 'cleaned_validation.csv', index=False)\n",
        "test.to_csv(dataset_directory + 'cleaned_test.csv', index=False)"
      ],
      "id": "a923f44e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyIiiXApSHcK"
      },
      "outputs": [],
      "source": [
        "# Load cleaned datasets\n",
        "cleaned_train = pd.read_csv(dataset_directory + 'cleaned_train.csv')\n",
        "cleaned_validation = pd.read_csv(dataset_directory + 'cleaned_validation.csv')\n",
        "cleaned_test = pd.read_csv(dataset_directory + 'cleaned_test.csv')"
      ],
      "id": "NyIiiXApSHcK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "436a3d68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf24402-e9f0-4c89-d5ce-c3162f74cf64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1381   563   160 ...     0     0     0]\n",
            " [   37  6931  6002 ...     0     0     0]\n",
            " [ 2693   481   163 ...     0     0     0]\n",
            " ...\n",
            " [  504  4448  5598 ...     0     0     0]\n",
            " [ 4700 12630   160 ...     0     0     0]\n",
            " [   37    71   809 ...   773  1581    97]]\n",
            "[[ 5499  8832 17397 ...     0     0     0]\n",
            " [  776   482 17087 ...     0     0     0]\n",
            " [   53   939   584 ...     0     0     0]\n",
            " ...\n",
            " [ 3483   767  3839 ...     0     0     0]\n",
            " [  640   208  2862 ...     0     0     0]\n",
            " [  515   128   744 ...     0     0     0]]\n",
            "[[  308  2210   545 ...     0     0     0]\n",
            " [ 2693  2232   227 ...     0     0     0]\n",
            " [22285 17981  7272 ...     0     0     0]\n",
            " ...\n",
            " [ 1116  2147  5879 ... 16618   247   618]\n",
            " [ 3225  4310  4268 ...     0     0     0]\n",
            " [  929   105 13451 ...     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "# Combine all the text data into one list\n",
        "text_data = list(cleaned_train['article']) + list(cleaned_validation['article']) + list(cleaned_test['article'])\n",
        "\n",
        "# Tokenize the text data\n",
        "vocab_size = 100000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(text_data)\n",
        "\n",
        "# Convert the text data into numerical tokens\n",
        "train_tokens = tokenizer.texts_to_sequences(cleaned_train['article'])\n",
        "val_tokens = tokenizer.texts_to_sequences(cleaned_validation['article'])\n",
        "test_tokens = tokenizer.texts_to_sequences(cleaned_test['article'])\n",
        "\n",
        "# Pad the sequences to ensure they all have the same length\n",
        "max_len = 512\n",
        "train_tokens_padded = pad_sequences(train_tokens, maxlen=max_len, padding='post', truncating='post')\n",
        "val_tokens_padded = pad_sequences(val_tokens, maxlen=max_len, padding='post', truncating='post')\n",
        "test_tokens_padded = pad_sequences(test_tokens, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Print the tokenized and padded sequences\n",
        "print(train_tokens_padded)\n",
        "print(val_tokens_padded)\n",
        "print(test_tokens_padded)"
      ],
      "id": "436a3d68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdb54f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3be71f0-3e79-44e9-aa7a-cb7727616027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    111964672   ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, None, 64)     49216       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    111964672   ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, None, 256),  328704      ['conv1d[0][0]']                 \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['embedding_1[0][0]',            \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " attention (Attention)          (None, None, 256)    0           ['lstm_1[0][0]',                 \n",
            "                                                                  'lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, None, 512)    0           ['lstm_1[0][0]',                 \n",
            "                                                                  'attention[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 437362  224366706   ['concatenate[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 449,199,282\n",
            "Trainable params: 449,199,282\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, None, 256)    111964672   ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, None, 256)    111964672   ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 256),        525312      ['embedding_2[0][0]']            \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 256),  525312      ['embedding_3[0][0]',            \n",
            "                                 (None, 256),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 256)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, None, 437362  112402034   ['lstm_3[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 337,382,002\n",
            "Trainable params: 337,382,002\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture for extractive summarization\n",
        "def extractive_summarization_model(input_dim, output_dim, hidden_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim, hidden_units, mask_zero=True)(encoder_inputs)\n",
        "    encoder_cnn = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(encoder_embedding)\n",
        "    encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_cnn)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(output_dim, hidden_units, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "    decoder_attention = Concatenate()([decoder_outputs, attention])\n",
        "    decoder_dense = Dense(output_dim, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_attention)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n",
        "\n",
        "# Define the model architecture for abstractive summarization\n",
        "def abstractive_summarization_model(input_dim, output_dim, hidden_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(input_dim, hidden_units, mask_zero=True)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(hidden_units, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(output_dim, hidden_units, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "    decoder_dense = Dense(output_dim, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model\n",
        "\n",
        "# Define the model parameters\n",
        "input_dim = len(tokenizer.word_index) + 1\n",
        "output_dim = len(tokenizer.word_index) + 1\n",
        "hidden_units = 256\n",
        "\n",
        "# Build the extractive summarization model\n",
        "extractive_model = extractive_summarization_model(input_dim, output_dim, hidden_units)\n",
        "extractive_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
        "\n",
        "# Build the abstractive summarization model\n",
        "abstractive_model = abstractive_summarization_model(input_dim, output_dim, hidden_units)\n",
        "abstractive_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
        "\n",
        "# Print the model summaries\n",
        "extractive_model.summary()\n",
        "abstractive_model.summary()"
      ],
      "id": "fdb54f68"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RJbB-p9IZzZ"
      },
      "outputs": [],
      "source": [
        "# Define the target data for extractive and abstractive models\n",
        "train_target_data = cleaned_train['article']\n",
        "val_target_data = cleaned_validation['article']\n",
        "\n",
        "# Tokenize the target data\n",
        "train_target_tokens = tokenizer.texts_to_sequences(train_target_data)\n",
        "val_target_tokens = tokenizer.texts_to_sequences(val_target_data)\n",
        "\n",
        "# Pad the target sequences to ensure they have the same length as input sequences\n",
        "train_target_tokens_padded = pad_sequences(train_target_tokens, maxlen=max_len, padding='post', truncating='post')\n",
        "val_target_tokens_padded = pad_sequences(val_target_tokens, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Convert target data to categorical format\n",
        "train_target_categorical = to_categorical(train_target_tokens_padded, num_classes=vocab_size)\n",
        "val_target_categorical = to_categorical(val_target_tokens_padded, num_classes=vocab_size)\n",
        "\n",
        "# Train the extractive summarization model\n",
        "extractive_model.fit([train_tokens_padded, train_tokens_padded], train_target_categorical,\n",
        "                     validation_data=([val_tokens_padded, val_tokens_padded], val_target_categorical),\n",
        "                     batch_size=32, epochs=10)\n",
        "\n",
        "# Train the abstractive summarization model\n",
        "abstractive_model.fit([train_tokens_padded, train_tokens_padded], train_tokens_padded,\n",
        "                      validation_data=([val_tokens_padded, val_tokens_padded], val_tokens_padded),\n",
        "                      batch_size=batch_size, epochs=epochs)\n"
      ],
      "id": "0RJbB-p9IZzZ"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}